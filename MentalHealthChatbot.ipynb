{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72a20f37-4c7b-475b-aee2-f7e5aa2177bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters:  752141\n",
      "Chapter 1\n",
      "INTRODUCTION TO\n",
      "COGNITIVE BEHAVIOR THERAPY\n",
      "A revolution in the fi eld of mental health wa\n"
     ]
    }
   ],
   "source": [
    "with open(\"therapy.txt\", \"r\", encoding=\"utf=8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of characters: \", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a96eac5-fbd0-478f-ba51-e70bd38f07e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144079\n",
      "['Chapter', '1', 'INTRODUCTION', 'TO', 'COGNITIVE', 'BEHAVIOR', 'THERAPY', 'A', 'revolution', 'in', 'the', 'fi', 'eld', 'of', 'mental', 'health', 'was', 'initiated', 'in', 'the', 'early', '1960s', 'by', 'Aaron', 'T', '.', 'Beck', ',', 'MD', ',']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28e86ce9-63a5-4256-a586-989c5c2a8d80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8896"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2ca62b8-9a92-492e-9ead-a6d9facc5c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('#1', 1)\n",
      "('#2', 2)\n",
      "('#3', 3)\n",
      "('#4', 4)\n",
      "('#5', 5)\n",
      "('#6', 6)\n",
      "('#7', 7)\n",
      "('&', 8)\n",
      "(\"'\", 9)\n",
      "('(', 10)\n",
      "(')', 11)\n",
      "('+', 12)\n",
      "(',', 13)\n",
      "('.', 14)\n",
      "('0', 15)\n",
      "('0%', 16)\n",
      "('0–10', 17)\n",
      "('0–100', 18)\n",
      "('0–100%', 19)\n",
      "('0’s', 20)\n",
      "('1', 21)\n",
      "('1%', 22)\n",
      "('10', 23)\n",
      "('10%', 24)\n",
      "('10-minute', 25)\n",
      "('10-point', 26)\n",
      "('100', 27)\n",
      "('100%', 28)\n",
      "('100–101', 29)\n",
      "('101', 30)\n",
      "('102', 31)\n",
      "('102f', 32)\n",
      "('102–105', 33)\n",
      "('103', 34)\n",
      "('104', 35)\n",
      "('104f', 36)\n",
      "('104–105', 37)\n",
      "('105', 38)\n",
      "('106', 39)\n",
      "('106–107', 40)\n",
      "('107', 41)\n",
      "('107–108', 42)\n",
      "('107–146', 43)\n",
      "('108', 44)\n",
      "('108–110', 45)\n",
      "('109', 46)\n",
      "('109–110', 47)\n",
      "('10:00', 48)\n",
      "('10:30', 49)\n",
      "('10th', 50)\n",
      "('10–11', 51)\n",
      "('10–15', 52)\n",
      "('11', 53)\n",
      "('110', 54)\n",
      "('110–112', 55)\n",
      "('111', 56)\n",
      "('112', 57)\n",
      "('112–117', 58)\n",
      "('113', 59)\n",
      "('113–141', 60)\n",
      "('114', 61)\n",
      "('115', 62)\n",
      "('116', 63)\n",
      "('116–117', 64)\n",
      "('116–131', 65)\n",
      "('117', 66)\n",
      "('117–118', 67)\n",
      "('118', 68)\n",
      "('118–120', 69)\n",
      "('119', 70)\n",
      "('119–120', 71)\n",
      "('11:00', 72)\n",
      "('11:30', 73)\n",
      "('11–12', 74)\n",
      "('12', 75)\n",
      "('120', 76)\n",
      "('120–121', 77)\n",
      "('120–122', 78)\n",
      "('121', 79)\n",
      "('121–122', 80)\n",
      "('122', 81)\n",
      "('123', 82)\n",
      "('1239–1252', 83)\n",
      "('123–124', 84)\n",
      "('124', 85)\n",
      "('124–125', 86)\n",
      "('125', 87)\n",
      "('126', 88)\n",
      "('127', 89)\n",
      "('127–128', 90)\n",
      "('128', 91)\n",
      "('129', 92)\n",
      "('129–130', 93)\n",
      "('12–1', 94)\n",
      "('12–13', 95)\n",
      "('13', 96)\n",
      "('130', 97)\n",
      "('1302–1304', 98)\n",
      "('130–133', 99)\n",
      "('131', 100)\n",
      "('131–152', 101)\n",
      "('132', 102)\n",
      "('133', 103)\n",
      "('133–134', 104)\n",
      "('134', 105)\n",
      "('134–135', 106)\n",
      "('135', 107)\n",
      "('135–136', 108)\n",
      "('136', 109)\n",
      "('137', 110)\n",
      "('137–138', 111)\n",
      "('137–140', 112)\n",
      "('138', 113)\n",
      "('138–139', 114)\n",
      "('139', 115)\n",
      "('139–140', 116)\n",
      "('13–14', 117)\n",
      "('14', 118)\n",
      "('140', 119)\n",
      "('140–141', 120)\n",
      "('140–142', 121)\n",
      "('140–153', 122)\n",
      "('141', 123)\n",
      "('142', 124)\n",
      "('142–143', 125)\n",
      "('142–147', 126)\n",
      "('143', 127)\n",
      "('143–144', 128)\n",
      "('143–147', 129)\n",
      "('144', 130)\n",
      "('144–145', 131)\n",
      "('144–156', 132)\n",
      "('145', 133)\n",
      "('145–146', 134)\n",
      "('146', 135)\n",
      "('147', 136)\n",
      "('147–148', 137)\n",
      "('148', 138)\n",
      "('148–150', 139)\n",
      "('149', 140)\n",
      "('14–15', 141)\n",
      "('14–16', 142)\n",
      "('15', 143)\n",
      "('150', 144)\n",
      "('150–151', 145)\n",
      "('151', 146)\n",
      "('151–152', 147)\n",
      "('152', 148)\n",
      "('152–153', 149)\n",
      "('153', 150)\n",
      "('153–155', 151)\n",
      "('154', 152)\n",
      "('154f', 153)\n",
      "('155', 154)\n",
      "('155–157', 155)\n",
      "('156', 156)\n",
      "('156f', 157)\n",
      "('157', 158)\n",
      "('158', 159)\n",
      "('158–159', 160)\n",
      "('159', 161)\n",
      "('159–161', 162)\n",
      "('159–162', 163)\n",
      "('15–16', 164)\n",
      "('16', 165)\n",
      "('160', 166)\n",
      "('160–166', 167)\n",
      "('161', 168)\n",
      "('161–162', 169)\n",
      "('162', 170)\n",
      "('162f', 171)\n",
      "('162–163', 172)\n",
      "('163', 173)\n",
      "('164', 174)\n",
      "('164–165', 175)\n",
      "('164–166', 176)\n",
      "('165', 177)\n",
      "('165f', 178)\n",
      "('166', 179)\n",
      "('167', 180)\n",
      "('167–169', 181)\n",
      "('168', 182)\n",
      "('168–169', 183)\n",
      "('169', 184)\n",
      "('169–170', 185)\n",
      "('17', 186)\n",
      "('170', 187)\n",
      "('170–171', 188)\n",
      "('170–176', 189)\n",
      "('171', 190)\n",
      "('171–176', 191)\n",
      "('172', 192)\n",
      "('172f', 193)\n",
      "('172–173', 194)\n",
      "('173', 195)\n",
      "('173–174', 196)\n",
      "('174', 197)\n",
      "('174–175', 198)\n",
      "('175', 199)\n",
      "('176', 200)\n",
      "('176–178', 201)\n",
      "('177', 202)\n",
      "('177–178', 203)\n",
      "('178', 204)\n",
      "('178–179', 205)\n",
      "('178–180', 206)\n",
      "('178–186', 207)\n",
      "('179', 208)\n",
      "('179–180', 209)\n",
      "('17–21', 210)\n",
      "('17–31', 211)\n",
      "('17–37', 212)\n",
      "('18', 213)\n",
      "('18-year-old', 214)\n",
      "('180', 215)\n",
      "('181', 216)\n",
      "('181f', 217)\n",
      "('181–182f', 218)\n",
      "('182', 219)\n",
      "('182f', 220)\n",
      "('182–184', 221)\n",
      "('183', 222)\n",
      "('184', 223)\n",
      "('184–186', 224)\n",
      "('185', 225)\n",
      "('185–186', 226)\n",
      "('186', 227)\n",
      "('187', 228)\n",
      "('187–197', 229)\n",
      "('188', 230)\n",
      "('188–191', 231)\n",
      "('189', 232)\n",
      "('18–19', 233)\n",
      "('19', 234)\n",
      "('190', 235)\n",
      "('191', 236)\n",
      "('192', 237)\n",
      "('192–193', 238)\n",
      "('192–194', 239)\n",
      "('192–197', 240)\n",
      "('193', 241)\n",
      "('193–194', 242)\n",
      "('194', 243)\n",
      "('195', 244)\n",
      "('1950s', 245)\n",
      "('195f', 246)\n",
      "('196', 247)\n",
      "('1960s', 248)\n",
      "('1962', 249)\n",
      "('1964', 250)\n",
      "('1964;', 251)\n",
      "('1967', 252)\n",
      "('196f', 253)\n",
      "('196–197', 254)\n",
      "('197', 255)\n",
      "('1970s', 256)\n",
      "('1974', 257)\n",
      "('1975', 258)\n",
      "('1975;', 259)\n",
      "('1976', 260)\n",
      "('1977', 261)\n",
      "('1978', 262)\n",
      "('1979', 263)\n",
      "('1979;', 264)\n",
      "('198', 265)\n",
      "('1980', 266)\n",
      "('1980;', 267)\n",
      "('1982', 268)\n",
      "('1985', 269)\n",
      "('1985;', 270)\n",
      "('1986', 271)\n",
      "('1987', 272)\n",
      "('1988', 273)\n",
      "('1989', 274)\n",
      "('1989;', 275)\n",
      "('199', 276)\n",
      "('1990', 277)\n",
      "('1990;', 278)\n",
      "('1991', 279)\n",
      "('1992', 280)\n",
      "('1992;', 281)\n",
      "('1993', 282)\n",
      "('1993;', 283)\n",
      "('1994', 284)\n",
      "('1996', 285)\n",
      "('1997', 286)\n",
      "('1998', 287)\n",
      "('1999', 288)\n",
      "('1999;', 289)\n",
      "('19–20', 290)\n",
      "('1:', 291)\n",
      "('1]', 292)\n",
      "('1–10', 293)\n",
      "('1–100', 294)\n",
      "('1–2', 295)\n",
      "('1’s', 296)\n",
      "('2', 297)\n",
      "('2/22', 298)\n",
      "('20', 299)\n",
      "('20%', 300)\n",
      "('200', 301)\n",
      "('2000', 302)\n",
      "('2000;', 303)\n",
      "('2001', 304)\n",
      "('2002', 305)\n",
      "('2002;', 306)\n",
      "('2003', 307)\n",
      "('2003;', 308)\n",
      "('2004', 309)\n",
      "('2004;', 310)\n",
      "('2005', 311)\n",
      "('2005;', 312)\n",
      "('2006', 313)\n",
      "('2007', 314)\n",
      "('2008', 315)\n",
      "('2008;', 316)\n",
      "('2009', 317)\n",
      "('2009;', 318)\n",
      "('200]', 319)\n",
      "('200f', 320)\n",
      "('201', 321)\n",
      "('2010', 322)\n",
      "('2010;', 323)\n",
      "('2010a', 324)\n",
      "('2010b', 325)\n",
      "('2011', 326)\n",
      "('201–203', 327)\n",
      "('201–204', 328)\n",
      "('201–205', 329)\n",
      "('202', 330)\n",
      "('202f', 331)\n",
      "('203', 332)\n",
      "('203f', 333)\n",
      "('203–204', 334)\n",
      "('204', 335)\n",
      "('204f', 336)\n",
      "('204–205', 337)\n",
      "('205', 338)\n",
      "('205–209', 339)\n",
      "('205–212', 340)\n",
      "('206', 341)\n",
      "('206–208', 342)\n",
      "('207', 343)\n",
      "('207–209', 344)\n",
      "('208', 345)\n",
      "('209', 346)\n",
      "('209–210', 347)\n",
      "('20:', 348)\n",
      "('20s', 349)\n",
      "('21', 350)\n",
      "('210', 351)\n",
      "('210–212', 352)\n",
      "('211', 353)\n",
      "('211–212', 354)\n",
      "('212', 355)\n",
      "('212–213', 356)\n",
      "('213', 357)\n",
      "('214', 358)\n",
      "('215', 359)\n",
      "('215–217', 360)\n",
      "('215–226', 361)\n",
      "('216', 362)\n",
      "('217', 363)\n",
      "('217–218', 364)\n",
      "('218', 365)\n",
      "('218–220', 366)\n",
      "('219', 367)\n",
      "('21–22', 368)\n",
      "('22', 369)\n",
      "('220', 370)\n",
      "('220–222', 371)\n",
      "('221', 372)\n",
      "('222', 373)\n",
      "('222–225', 374)\n",
      "('223', 375)\n",
      "('224', 376)\n",
      "('224–225', 377)\n",
      "('225', 378)\n",
      "('226', 379)\n",
      "('227', 380)\n",
      "('228', 381)\n",
      "('229', 382)\n",
      "('22–23', 383)\n",
      "('23', 384)\n",
      "('230', 385)\n",
      "('230–231', 386)\n",
      "('231', 387)\n",
      "('231–232', 388)\n",
      "('232', 389)\n",
      "('233', 390)\n",
      "('233f', 391)\n",
      "('233–234', 392)\n",
      "('234', 393)\n",
      "('235', 394)\n",
      "('235–239', 395)\n",
      "('235–240', 396)\n",
      "('236', 397)\n",
      "('237', 398)\n",
      "('238', 399)\n",
      "('239', 400)\n",
      "('239–241', 401)\n",
      "('23–25', 402)\n",
      "('24', 403)\n",
      "('240', 404)\n",
      "('241', 405)\n",
      "('241–242', 406)\n",
      "('242', 407)\n",
      "('242f', 408)\n",
      "('242–246', 409)\n",
      "('242–253', 410)\n",
      "('243', 411)\n",
      "('243f', 412)\n",
      "('244', 413)\n",
      "('245', 414)\n",
      "('246', 415)\n",
      "('246–247', 416)\n",
      "('247', 417)\n",
      "('247–248', 418)\n",
      "('248', 419)\n",
      "('248–251', 420)\n",
      "('248–255', 421)\n",
      "('249', 422)\n",
      "('25', 423)\n",
      "('25%', 424)\n",
      "('250', 425)\n",
      "('251', 426)\n",
      "('251–255', 427)\n",
      "('252', 428)\n",
      "('253', 429)\n",
      "('254', 430)\n",
      "('255', 431)\n",
      "('256', 432)\n",
      "('256–258', 433)\n",
      "('257', 434)\n",
      "('258', 435)\n",
      "('258–260', 436)\n",
      "('259', 437)\n",
      "('26', 438)\n",
      "('260', 439)\n",
      "('260–263', 440)\n",
      "('261', 441)\n",
      "('261–263', 442)\n",
      "('262', 443)\n",
      "('263', 444)\n",
      "('263–264', 445)\n",
      "('264', 446)\n",
      "('265', 447)\n",
      "('265–266', 448)\n",
      "('265–267', 449)\n",
      "('266', 450)\n",
      "('267', 451)\n",
      "('267–268', 452)\n",
      "('268', 453)\n",
      "('268–270', 454)\n",
      "('268–272', 455)\n",
      "('269', 456)\n",
      "('269f', 457)\n",
      "('26–27', 458)\n",
      "('27', 459)\n",
      "('270', 460)\n",
      "('271', 461)\n",
      "('272', 462)\n",
      "('272–274', 463)\n",
      "('273', 464)\n",
      "('274', 465)\n",
      "('274–276', 466)\n",
      "('275', 467)\n",
      "('276', 468)\n",
      "('277', 469)\n",
      "('277–279', 470)\n",
      "('277–280', 471)\n",
      "('278', 472)\n",
      "('279', 473)\n",
      "('279–280', 474)\n",
      "('27–28', 475)\n",
      "('28', 476)\n",
      "('280', 477)\n",
      "('280–283', 478)\n",
      "('280–289', 479)\n",
      "('281', 480)\n",
      "('281–292', 481)\n",
      "('282', 482)\n",
      "('283', 483)\n",
      "('283–297', 484)\n",
      "('284', 485)\n",
      "('284–285', 486)\n",
      "('285', 487)\n",
      "('285–287', 488)\n",
      "('286', 489)\n",
      "('287', 490)\n",
      "('287–288', 491)\n",
      "('288', 492)\n",
      "('288–289', 493)\n",
      "('289', 494)\n",
      "('289–290', 495)\n",
      "('289–293', 496)\n",
      "('29', 497)\n",
      "('290', 498)\n",
      "('290–293', 499)\n",
      "('291', 500)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 500:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a5d5123-1d39-43dc-ae22-aeefe8bcd788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "        #s is token and i is token ID, so flipping it\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fa42672-8975-4243-aeff-74354aeda4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1674, 3268, 2368, 5451, 6396, 4922, 7434, 8116, 4647, 5261, 6787, 3161, 7442, 7631, 5105, 7574, 3532, 7462, 3161, 1674, 5220, 2925, 7459, 8257, 6309, 8517, 7462, 5686, 8583, 8143]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"I ask Sally in our first session to enumerate her problems and\n",
    "set specific goals so both she and I have a shared understanding of what\n",
    "she is working toward\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d0d087e-b123-45cb-9197-10c2d43bccc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I ask Sally in our first session to enumerate her problems and set specific goals so both she and I have a shared understanding of what she is working toward'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c266a51-de26-4476-929c-29b070d11d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Special Content Tokens\n",
    "#|<unk>| and |<endoftext>| - for unknown words and process data in better way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e141d20-080a-4bfc-a603-1e8dc97edc76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8898"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c48fc255-d6a5-4262-9471-c0201ede0375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('•', 8893)\n",
      "('\\uf090', 8894)\n",
      "('\\uf0d2', 8895)\n",
      "('<|endoftext|>', 8896)\n",
      "('<|unk|>', 8897)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d590bf37-258e-4402-9fb4-b2f145316dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "        #s is token and i is token ID, so flipping it\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int\n",
    "                       else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fbde320-048e-41eb-97de-0b795ef7d975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you feeling <|endoftext|> I dislike therapy.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, how are you feeling\"\n",
    "text2 = \"I dislike therapy.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6881da7e-870d-4f26-991f-d581b4d04439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, how are you feeling <|endoftext|> I dislike therapy.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53849d82-45b8-40dc-b659-34a6dadfab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42d6fccb-e252-484e-a522-7a2efd33664c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Obtaining dependency information for tiktoken from https://files.pythonhosted.org/packages/4d/ae/4613a59a2a48e761c5161237fc850eb470b4bb93696db89da51b79a871f1/tiktoken-0.9.0-cp311-cp311-macosx_10_12_x86_64.whl.metadata\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./anaconda3/lib/python3.11/site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./anaconda3/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Downloading tiktoken-0.9.0-cp311-cp311-macosx_10_12_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3454010e-17c0-43f3-9c7a-a24cd155fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb413e9c-7d5a-4657-a78c-324c77c92607",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f104a72-2cea-46ba-ae32-1fb4ade06b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 703, 389, 345, 4203, 30, 220, 50256, 314, 1842, 12734]\n"
     ]
    }
   ],
   "source": [
    "text = (\"Hello, how are you feeling? <|endoftext|> I love flowers\")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5758de6d-082e-4f16-bfe2-55568aa1ac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating input target pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c96c2b9d-8ee3-4afe-b8de-4807a6572d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186730\n"
     ]
    }
   ],
   "source": [
    "with open(\"therapy.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d54a520-ef97-4151-8ba2-7257ea72f6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14126, 352, 198, 1268] [352, 198, 1268, 5446]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "\n",
    "x = enc_text[:context_size]\n",
    "y = enc_text[1:context_size+1]\n",
    "\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64f30da9-314c-4914-942c-a00e673a1a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14126] ---> 352\n",
      "[14126, 352] ---> 198\n",
      "[14126, 352, 198] ---> 1268\n",
      "[14126, 352, 198, 1268] ---> 5446\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_text[:i]\n",
    "    desired = enc_text[i]\n",
    "\n",
    "    print(context,\"--->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a715116-449a-4f02-bac1-8ffdb1e15910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement a Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7b1c44f-bab5-4539-9f5c-da55b599ac63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/3f/14/e105b8ef6d324e789c1589e95cb0ab63f3e07c2216d68b1178b7c21b7d2a/torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in ./anaconda3/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Obtaining dependency information for typing-extensions>=4.8.0 from https://files.pythonhosted.org/packages/69/e0/552843e0d356fbb5256d21449fa957fa4eff3bbc135a74a691ee70c7c5da/typing_extensions-4.14.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in ./anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in ./anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./anaconda3/lib/python3.11/site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "Installing collected packages: typing-extensions, torch\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "Successfully installed torch-2.2.2 typing-extensions-4.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "adc1c1de-a1c4-4581-bbd9-9be272bb4ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3d89a42-cf05-4ed9-a42e-7fdb3cbda6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce7218bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e03d8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"therapy.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a104fd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[14126,   352,   198,  1268]]), tensor([[ 352,  198, 1268, 5446]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "\n",
    "#more structured data loader\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "75b9bb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[14126,   352,   198,  1268],\n",
      "        [ 5446, 28644,  2849,  5390],\n",
      "        [  198,    34,  7730,    45],\n",
      "        [ 2043,  9306,  9348,  7801],\n",
      "        [12861,  1581,  2320,  1137],\n",
      "        [ 2969,    56,   198,    32],\n",
      "        [ 5854,   287,   262, 25912],\n",
      "        [18441,   286,  5110,  1535]])\n",
      "Targets:\n",
      " tensor([[  352,   198,  1268,  5446],\n",
      "        [28644,  2849,  5390,   198],\n",
      "        [   34,  7730,    45,  2043],\n",
      "        [ 9306,  9348,  7801, 12861],\n",
      "        [ 1581,  2320,  1137,  2969],\n",
      "        [   56,   198,    32,  5854],\n",
      "        [  287,   262, 25912, 18441],\n",
      "        [  286,  5110,  1535,   373]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=4, stride=4, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"Targets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8555861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Token Embeddings - assigning values to words (aka vector embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7fe51ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7849f141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===================================---------------] 72.0% 1196.5/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====================================-------------] 74.6% 1240.9/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c49c39d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.07421875e-01 -2.01171875e-01  1.23046875e-01  2.11914062e-01\n",
      " -9.13085938e-02  2.16796875e-01 -1.31835938e-01  8.30078125e-02\n",
      "  2.02148438e-01  4.78515625e-02  3.66210938e-02 -2.45361328e-02\n",
      "  2.39257812e-02 -1.60156250e-01 -2.61230469e-02  9.71679688e-02\n",
      " -6.34765625e-02  1.84570312e-01  1.70898438e-01 -1.63085938e-01\n",
      " -1.09375000e-01  1.49414062e-01 -4.65393066e-04  9.61914062e-02\n",
      "  1.68945312e-01  2.60925293e-03  8.93554688e-02  6.49414062e-02\n",
      "  3.56445312e-02 -6.93359375e-02 -1.46484375e-01 -1.21093750e-01\n",
      " -2.27539062e-01  2.45361328e-02 -1.24511719e-01 -3.18359375e-01\n",
      " -2.20703125e-01  1.30859375e-01  3.66210938e-02 -3.63769531e-02\n",
      " -1.13281250e-01  1.95312500e-01  9.76562500e-02  1.26953125e-01\n",
      "  6.59179688e-02  6.93359375e-02  1.02539062e-02  1.75781250e-01\n",
      " -1.68945312e-01  1.21307373e-03 -2.98828125e-01 -1.15234375e-01\n",
      "  5.66406250e-02 -1.77734375e-01 -2.08984375e-01  1.76757812e-01\n",
      "  2.38037109e-02 -2.57812500e-01 -4.46777344e-02  1.88476562e-01\n",
      "  5.51757812e-02  5.02929688e-02 -1.06933594e-01  1.89453125e-01\n",
      " -1.16210938e-01  8.49609375e-02 -1.71875000e-01  2.45117188e-01\n",
      " -1.73828125e-01 -8.30078125e-03  4.56542969e-02 -1.61132812e-02\n",
      "  1.86523438e-01 -6.05468750e-02 -4.17480469e-02  1.82617188e-01\n",
      "  2.20703125e-01 -1.22558594e-01 -2.55126953e-02 -3.08593750e-01\n",
      "  9.13085938e-02  1.60156250e-01  1.70898438e-01  1.19628906e-01\n",
      "  7.08007812e-02 -2.64892578e-02 -3.08837891e-02  4.06250000e-01\n",
      " -1.01562500e-01  5.71289062e-02 -7.26318359e-03 -9.17968750e-02\n",
      " -1.50390625e-01 -2.55859375e-01  2.16796875e-01 -3.63769531e-02\n",
      "  2.24609375e-01  8.00781250e-02  1.56250000e-01  5.27343750e-02\n",
      "  1.50390625e-01 -1.14746094e-01 -8.64257812e-02  1.19140625e-01\n",
      " -7.17773438e-02  2.73437500e-01 -1.64062500e-01  7.29370117e-03\n",
      "  4.21875000e-01 -1.12792969e-01 -1.35742188e-01 -1.31835938e-01\n",
      " -1.37695312e-01 -7.66601562e-02  6.25000000e-02  4.98046875e-02\n",
      " -1.91406250e-01 -6.03027344e-02  2.27539062e-01  5.88378906e-02\n",
      " -3.24218750e-01  5.41992188e-02 -1.35742188e-01  8.17871094e-03\n",
      " -5.24902344e-02 -1.74713135e-03 -9.81445312e-02 -2.86865234e-02\n",
      "  3.61328125e-02  2.15820312e-01  5.98144531e-02 -3.08593750e-01\n",
      " -2.27539062e-01  2.61718750e-01  9.86328125e-02 -5.07812500e-02\n",
      "  1.78222656e-02  1.31835938e-01 -5.35156250e-01 -1.81640625e-01\n",
      "  1.38671875e-01 -3.10546875e-01 -9.71679688e-02  1.31835938e-01\n",
      " -1.16210938e-01  7.03125000e-02  2.85156250e-01  3.51562500e-02\n",
      " -1.01562500e-01 -3.75976562e-02  1.41601562e-01  1.42578125e-01\n",
      " -5.68847656e-02  2.65625000e-01 -2.09960938e-01  9.64355469e-03\n",
      " -6.68945312e-02 -4.83398438e-02 -6.10351562e-02  2.45117188e-01\n",
      " -9.66796875e-02  1.78222656e-02 -1.27929688e-01 -4.78515625e-02\n",
      " -7.26318359e-03  1.79687500e-01  2.78320312e-02 -2.10937500e-01\n",
      " -1.43554688e-01 -1.27929688e-01  1.73339844e-02 -3.60107422e-03\n",
      " -2.04101562e-01  3.63159180e-03 -1.19628906e-01 -6.15234375e-02\n",
      "  5.93261719e-02 -3.23486328e-03 -1.70898438e-01 -3.14941406e-02\n",
      " -8.88671875e-02 -2.89062500e-01  3.44238281e-02 -1.87500000e-01\n",
      "  2.94921875e-01  1.58203125e-01 -1.19628906e-01  7.61718750e-02\n",
      "  6.39648438e-02 -4.68750000e-02 -6.83593750e-02  1.21459961e-02\n",
      " -1.44531250e-01  4.54101562e-02  3.68652344e-02  3.88671875e-01\n",
      "  1.45507812e-01 -2.55859375e-01 -4.46777344e-02 -1.33789062e-01\n",
      " -1.38671875e-01  6.59179688e-02  1.37695312e-01  1.14746094e-01\n",
      "  2.03125000e-01 -4.78515625e-02  1.80664062e-02 -8.54492188e-02\n",
      " -2.48046875e-01 -3.39843750e-01 -2.83203125e-02  1.05468750e-01\n",
      " -2.14843750e-01 -8.74023438e-02  7.12890625e-02  1.87500000e-01\n",
      " -1.12304688e-01  2.73437500e-01 -3.26171875e-01 -1.77734375e-01\n",
      " -4.24804688e-02 -2.69531250e-01  6.64062500e-02 -6.88476562e-02\n",
      " -1.99218750e-01 -7.03125000e-02 -2.43164062e-01 -3.66210938e-02\n",
      " -7.37304688e-02 -1.77734375e-01  9.17968750e-02 -1.25000000e-01\n",
      " -1.65039062e-01 -3.57421875e-01 -2.85156250e-01 -1.66992188e-01\n",
      "  1.97265625e-01 -1.53320312e-01  2.31933594e-02  2.06054688e-01\n",
      "  1.80664062e-01 -2.74658203e-02 -1.92382812e-01 -9.61914062e-02\n",
      " -1.06811523e-02 -4.73632812e-02  6.54296875e-02 -1.25732422e-02\n",
      "  1.78222656e-02 -8.00781250e-02 -2.59765625e-01  9.37500000e-02\n",
      " -7.81250000e-02  4.68750000e-02 -2.22167969e-02  1.86767578e-02\n",
      "  3.11279297e-02  1.04980469e-02 -1.69921875e-01  2.58789062e-02\n",
      " -3.41796875e-02 -1.44042969e-02 -5.46875000e-02 -8.78906250e-02\n",
      "  1.96838379e-03  2.23632812e-01 -1.36718750e-01  1.75781250e-01\n",
      " -1.63085938e-01  1.87500000e-01  3.44238281e-02 -5.63964844e-02\n",
      " -2.27689743e-05  4.27246094e-02  5.81054688e-02 -1.07910156e-01\n",
      " -3.88183594e-02 -2.69531250e-01  3.34472656e-02  9.81445312e-02\n",
      "  5.63964844e-02  2.23632812e-01 -5.49316406e-02  1.46484375e-01\n",
      "  5.93261719e-02 -2.19726562e-01  6.39648438e-02  1.66015625e-02\n",
      "  4.56542969e-02  3.26171875e-01 -3.80859375e-01  1.70898438e-01\n",
      "  5.66406250e-02 -1.04492188e-01  1.38671875e-01 -1.57226562e-01\n",
      "  3.23486328e-03 -4.80957031e-02 -2.48046875e-01 -6.20117188e-02]\n"
     ]
    }
   ],
   "source": [
    "word_vectors = model\n",
    "\n",
    "print(word_vectors['computer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0b073b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118193507194519), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321839332581), ('kings', 0.5236844420433044), ('Queen_Consort', 0.5235945582389832), ('queens', 0.5181134343147278), ('sultan', 0.5098593831062317), ('monarchy', 0.5087411999702454)]\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors.most_similar(positive=['king', 'woman'], negative=['man'], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762b7a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
